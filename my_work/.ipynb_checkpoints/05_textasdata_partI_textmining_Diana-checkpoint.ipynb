{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## activate virtural environment\n",
    "## conda activate qss20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/zixuanwang/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/zixuanwang/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load packages \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "## nltk imports\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "## uncomment and download if this is your first \n",
    "## time running \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "## sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "## specify to print all output in a call\n",
    "## and not just first\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## spacy --- if you get an error at the load step\n",
    "## need to download en_core_web_sm (google)\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>name_upper</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2539</td>\n",
       "      <td>Clean &amp; quiet apt home by the park</td>\n",
       "      <td>CLEAN &amp; QUIET APT HOME BY THE PARK</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2595</td>\n",
       "      <td>Skylit Midtown Castle</td>\n",
       "      <td>SKYLIT MIDTOWN CASTLE</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3647</td>\n",
       "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
       "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3831</td>\n",
       "      <td>Cozy Entire Floor of Brownstone</td>\n",
       "      <td>COZY ENTIRE FLOOR OF BROWNSTONE</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5022</td>\n",
       "      <td>Entire Apt: Spacious Studio/Loft by central park</td>\n",
       "      <td>ENTIRE APT: SPACIOUS STUDIO/LOFT BY CENTRAL PARK</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              name  \\\n",
       "0  2539                Clean & quiet apt home by the park   \n",
       "1  2595                             Skylit Midtown Castle   \n",
       "2  3647               THE VILLAGE OF HARLEM....NEW YORK !   \n",
       "3  3831                   Cozy Entire Floor of Brownstone   \n",
       "4  5022  Entire Apt: Spacious Studio/Loft by central park   \n",
       "\n",
       "                                         name_upper neighbourhood_group  price  \n",
       "0                CLEAN & QUIET APT HOME BY THE PARK            Brooklyn    149  \n",
       "1                             SKYLIT MIDTOWN CASTLE           Manhattan    225  \n",
       "2               THE VILLAGE OF HARLEM....NEW YORK !           Manhattan    150  \n",
       "3                   COZY ENTIRE FLOOR OF BROWNSTONE            Brooklyn     89  \n",
       "4  ENTIRE APT: SPACIOUS STUDIO/LOFT BY CENTRAL PARK           Manhattan     80  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48895 entries, 0 to 48894\n",
      "Data columns (total 5 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   id                   48895 non-null  int64 \n",
      " 1   name                 48879 non-null  object\n",
      " 2   name_upper           48879 non-null  object\n",
      " 3   neighbourhood_group  48895 non-null  object\n",
      " 4   price                48895 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "## if working from within the repo, can use this relative path\n",
    "path_todata = \"../public_data/airbnb_text.csv\"\n",
    "\n",
    "## load data\n",
    "ab = pd.read_csv(path_todata)\n",
    "ab.head()\n",
    "ab.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual approach 1: look for a single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_mention_cozy</th>\n",
       "      <th>mention_cozy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bronx</th>\n",
       "      <td>89.231088</td>\n",
       "      <td>74.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brooklyn</th>\n",
       "      <td>128.175441</td>\n",
       "      <td>91.130224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manhattan</th>\n",
       "      <td>204.109775</td>\n",
       "      <td>129.917140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Queens</th>\n",
       "      <td>102.596682</td>\n",
       "      <td>80.344388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Staten Island</th>\n",
       "      <td>120.650307</td>\n",
       "      <td>74.319149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     no_mention_cozy  mention_cozy\n",
       "neighbourhood_group                               \n",
       "Bronx                      89.231088     74.214286\n",
       "Brooklyn                  128.175441     91.130224\n",
       "Manhattan                 204.109775    129.917140\n",
       "Queens                    102.596682     80.344388\n",
       "Staten Island             120.650307     74.319149"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using the `name_upper` var, look at where reviews mention cozy\n",
    "ab['is_cozy'] = np.where(ab.name_upper.str.contains(\"COZY\"), True, False)\n",
    "\n",
    "## find the mean price by neighborhood and whether cozy\n",
    "mp = pd.DataFrame(ab.groupby(['is_cozy', 'neighbourhood_group'])['price'].mean())\n",
    "\n",
    "## reshape to wide format so that each borough is row\n",
    "## and one col is the mean price for listings that describe\n",
    "## the place as cozy; other col is mean price for listings\n",
    "## without that word\n",
    "mp_wide = pd.pivot_table(mp, index = ['neighbourhood_group'],\n",
    "                        columns = ['is_cozy'])\n",
    "\n",
    "mp_wide.columns = ['no_mention_cozy', 'mention_cozy']\n",
    "\n",
    "mp_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual approach 2: score based on dictionary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COZY', 'LITTLE']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## construct dictionary\n",
    "space_indicators = {'small': ['COZY', 'COMFY', 'LITTLE', 'SMALL'],\n",
    "                   'large': ['SPACIOUS', 'LARGE', 'HUGE', 'GIANT']}\n",
    "\n",
    "\n",
    "## for each listing, find the number of occurrences\n",
    "## of words in each key\n",
    "\n",
    "### first, let's test with one listing\n",
    "practice_listing = \"NICE AND COZY LITTLE APT AVAILABLE\"\n",
    "\n",
    "### splitting that string at space and looking at overlap with each key\n",
    "### first, look at overlap with the list containing words for small\n",
    "words_overlap_small = [word \n",
    "                    for word in practice_listing.split(\" \") if \n",
    "                      word in space_indicators['small']]\n",
    "words_overlap_small\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### then, look at overlap with the list containing words for large\n",
    "words_overlap_large = [word for word in practice_listing.split(\" \") if \n",
    "                      word in space_indicators['large']]\n",
    "words_overlap_large\n",
    "\n",
    "### could then take length as a fraction of all words\n",
    "len(words_overlap_small)/len(practice_listing.split(\" \"))\n",
    "len(words_overlap_large)/len(practice_listing.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a chill apt next to the subway in LES Chinatown'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## specify example\n",
    "example_for_tag = \"This is a chill apt next to the subway in LES Chinatown\"\n",
    "example_for_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'chill',\n",
       " 'apt',\n",
       " 'next',\n",
       " 'to',\n",
       " 'the',\n",
       " 'subway',\n",
       " 'in',\n",
       " 'LES',\n",
       " 'Chinatown']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('chill', 'NN'),\n",
       " ('apt', 'JJ'),\n",
       " ('next', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('subway', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('LES', 'NNP'),\n",
       " ('Chinatown', 'NNP')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## try part of speech tagging using nltk\n",
    "tokens = word_tokenize(example_for_tag) # Generate list of tokens\n",
    "tokens\n",
    "\n",
    "tokens_pos = pos_tag(tokens) # generate part of speech tags for those tokens\n",
    " \n",
    "## returns a list of tuples\n",
    "## first element in tuple is a word\n",
    "## second element in tuple is the part of speech\n",
    "#for one_tok in tokens_pos:\n",
    " #   print(one_tok)\n",
    "tokens_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LES', 'Chinatown']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['chill', 'apt', 'next', 'subway']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use list iteration to extract proper nouns (NNP)\n",
    "## i'm first checking if the second element in the tuple\n",
    "## is equal to NNP\n",
    "## if so, i'm returning the first element in the tuple (the \n",
    "## actual word)\n",
    "all_prop_noun = [one_tok[0] for one_tok in tokens_pos \n",
    "                if one_tok[1] == \"NNP\"]\n",
    "all_prop_noun\n",
    "\n",
    "all_adj_noun = [one_tok[0] for one_tok in tokens_pos \n",
    "                if one_tok[1] == \"JJ\" or \n",
    "               one_tok[1] == \"NN\"]\n",
    "all_adj_noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modified from a real tweet\n",
    "\n",
    "## tweet\n",
    "d_tweet = \"\"\"We’ll be hosting on-campus COVID-19 booster clinics \n",
    "at Dartmouth College in New Hampshire from 9 a.m. to 6 p.m. on\n",
    "Monday, Jan. 10, and Tuesday, Jan. 11, at\n",
    "Alumni Hall in the Hopkins Center. For information on how to\n",
    "register and additional winter updates, head to\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\nIt was the worse of time\\nIt was the best of time\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'string'\n",
    "\n",
    "'''\n",
    "It was the worse of time\n",
    "It was the best of time\n",
    "'''\n",
    "# longer string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "spacy_dtweet = nlp(d_tweet) # convert d_tweet to spacy object\n",
    "print(type(spacy_dtweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(COVID-19,\n",
       " Dartmouth College,\n",
       " New Hampshire,\n",
       " 9 a.m. to 6 p.m.,\n",
       " Monday, Jan. 10,\n",
       " Tuesday, Jan. 11,\n",
       " Alumni Hall,\n",
       " the Hopkins Center,\n",
       " winter)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "COVID-19"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_dtweet.ents # all entities detected\n",
    "spacy_dtweet.ents[0] # a spacy token object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: COVID-19; NER tag: ORG\n",
      "Entity: Dartmouth College; NER tag: ORG\n",
      "Entity: New Hampshire; NER tag: GPE\n",
      "Entity: 9 a.m. to 6 p.m.; NER tag: TIME\n",
      "Entity: Monday, Jan. 10; NER tag: DATE\n",
      "Entity: Tuesday, Jan. 11; NER tag: DATE\n",
      "Entity: Alumni Hall; NER tag: FAC\n",
      "Entity: the Hopkins Center; NER tag: ORG\n",
      "Entity: winter; NER tag: DATE\n"
     ]
    }
   ],
   "source": [
    "## try a couple variations\n",
    "for one_tok in spacy_dtweet.ents:\n",
    "    print(\"Entity: \" + one_tok.text + \"; NER tag: \" + one_tok.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1\n",
    "\n",
    "Play around with different variations of the Dartmouth tweet and look at the results. For instance, try the following:\n",
    "\n",
    "- What happens if you abbreviate New Hampshire to NH?\n",
    "- What happens if you add the word Pfizer before COVID-19?\n",
    "- What entities seem misclassified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "d_tweet = \"\"\"We’ll be hosting on-campus Pfizer COVID-19 booster clinics \n",
    "at Dartmouth College in NH from 9 a.m. to 6 p.m. on\n",
    "Monday, Jan. 10, and Tuesday, Jan. 11, at\n",
    "Alumni Hall in the Hopkins Center. For information on how to\n",
    "register and additional winter updates, head to\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "spacy_dtweet = nlp(d_tweet) # convert d_tweet to spacy object\n",
    "print(type(spacy_dtweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Dartmouth College; NER tag: ORG\n",
      "Entity: NH; NER tag: ORG\n",
      "Entity: 9 a.m. to 6 p.m.; NER tag: TIME\n",
      "Entity: Monday, Jan. 10; NER tag: DATE\n",
      "Entity: Tuesday, Jan. 11; NER tag: DATE\n",
      "Entity: Alumni Hall; NER tag: FAC\n",
      "Entity: the Hopkins Center; NER tag: ORG\n",
      "Entity: winter; NER tag: DATE\n"
     ]
    }
   ],
   "source": [
    "for one_tok in spacy_dtweet.ents:\n",
    "    print(\"Entity: \" + one_tok.text + \"; NER tag: \" + one_tok.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2\n",
    "\n",
    "How do we generalize from doing Named Entity Recognition one just one string to doing NER on a whole column that contains strings? By defining and executing a **function**, of course!\n",
    "\n",
    "Using the following sample of strings from airbnb listing names, define a function that takes in one airbnb string and does the following:\n",
    "\n",
    "- iterates over entities in that string (list comprehension would work well for this)\n",
    "- checks if each label indicates a place\n",
    "- returns the text of each original place/entity\n",
    "\n",
    "Then execute the function on the example strings through iteration (again, list comprehension works well here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10                    Beautiful 1br on Upper West Side\n",
       "11                     Central Manhattan/near Broadway\n",
       "12      Lovely Room 1, Garden, Best Area, Legal rental\n",
       "13    Wonderful Guest Bedroom in Manhattan for SINGLES\n",
       "14                       West Village Nest - Superhost\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## for runtime purposes, take a sample of the airbnb listing names\n",
    "ab_name_examples = ab.name[10:15]\n",
    "ab_name_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10                                 []\n",
       "11                [Central Manhattan]\n",
       "12                                 []\n",
       "13                        [Manhattan]\n",
       "14    [West Village Nest - Superhost]\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your function code here\n",
    "\n",
    "def find_entity(string):\n",
    "    spacy_string = nlp(string)\n",
    "    return [ent.text for ent in spacy_string.ents if (ent.label_ == \"GPE\") | (ent.label_ == \"LOC\")]\n",
    "\n",
    "ab_name_examples.apply(find_entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the default scorer on a few example phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'vaderSentiment.vaderSentiment.SentimentIntensityAnalyzer'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.641, 'pos': 0.359, 'compound': 0.4215}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## initialize a scorer\n",
    "sent_obj = SentimentIntensityAnalyzer()\n",
    "print(type(sent_obj))\n",
    "## score one listing\n",
    "practice_listing = \"NICE AND COZY LITTLE APT AVAILABLE\"\n",
    "sentiment_example = sent_obj.polarity_scores(practice_listing)\n",
    "sentiment_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding phrase with word terrible and score\n",
    "practice_listing_2 = \"NICE AND COZY LITTLE APT AVAILABLE. REALLY TERRIBLE VIEW.\"\n",
    "sentiment_example_2 = sent_obj.polarity_scores(practice_listing_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding phrase about rats; bad but might not be in scoring dictionary\n",
    "practice_listing_3 = \"NICE AND COZY LITTLE APT AVAILABLE. HAS RATS THOUGH.\"\n",
    "sentiment_example_3 = sent_obj.polarity_scores(practice_listing_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String: NICE AND COZY LITTLE APT AVAILABLE scored as:\n",
      "{'neg': 0.0, 'neu': 0.641, 'pos': 0.359, 'compound': 0.4215}\n",
      "String: NICE AND COZY LITTLE APT AVAILABLE. REALLY TERRIBLE VIEW. scored as:\n",
      "{'neg': 0.257, 'neu': 0.531, 'pos': 0.212, 'compound': -0.1513}\n",
      "String: NICE AND COZY LITTLE APT AVAILABLE. HAS RATS THOUGH. scored as:\n",
      "{'neg': 0.0, 'neu': 0.741, 'pos': 0.259, 'compound': 0.4215}\n"
     ]
    }
   ],
   "source": [
    "## summarize all 3\n",
    "print(\"String: \" + practice_listing + \" scored as:\\n\" + str(sentiment_example))\n",
    "print(\"String: \" + practice_listing_2 + \" scored as:\\n\" + str(sentiment_example_2))\n",
    "print(\"String: \" + practice_listing_3 + \" scored as:\\n\" + str(sentiment_example_3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the dictionary with manually-added words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sent_obj.lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## lexicon is a dictionary where the key\n",
    "## is the word\n",
    "## the value is the score (negative = negative)\n",
    "## here, i'm benchmarking the negativity of the\n",
    "## rodents to the negativity of the word aversion\n",
    "sent_obj.lexicon['aversion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After lexicon update: NICE AND COZY LITTLE APT AVAILABLE. HAS RATS THOUGH. scored as:\n",
      "{'neg': 0.228, 'neu': 0.551, 'pos': 0.22, 'compound': -0.0258}\n"
     ]
    }
   ],
   "source": [
    "## create a dictionary with \n",
    "## negative scores for pests\n",
    "pest_words = {\n",
    "    'rat': -1.9,\n",
    "    'rats': -1.9,\n",
    "    'mice': -1.9,\n",
    "    'mouse': -1.9,\n",
    "    'roach': -1.9,\n",
    "    'cockroach': -1.9\n",
    "}\n",
    "\n",
    "\n",
    "## initiate new sentiment object\n",
    "## so that we don't alter old one\n",
    "## use.update to add new words\n",
    "new_si = SentimentIntensityAnalyzer()\n",
    "new_si.lexicon.update(pest_words)\n",
    "\n",
    "## try re-scoring the third example\n",
    "## see negative\n",
    "print(\"After lexicon update: \" + practice_listing_3 + \" scored as:\\n\" + \\\n",
    "      str(new_si.polarity_scores(practice_listing_3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
