{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised machine learning: Decision Trees and optimization\n",
    "## Multi-class classification with text data\n",
    "\n",
    "## Outline\n",
    "\n",
    "* [Load and inspect dataset](#dataset)\n",
    "    * [Import modules](#import)\n",
    "    * [Read and inspect data](#dataset)\n",
    "    * [Preprocess data](#preprocess)\n",
    "    * [Vectorization](#vectorize)\n",
    "    * [Divide data into training and test sets](#split)\n",
    "* [More classification with supervised machine learning](#supervised)\n",
    "    * [Train decision tree model](#model)\n",
    "    * [More model evaluation](#evaluate)\n",
    "    * [Cross-validation](#cv)\n",
    "    * [Optimize parameters with grid search](#gridsearch)\n",
    "    * [Train and optimize Random Forest model](#RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation<a id='preparation'></a>\n",
    "\n",
    "## Import modules<a id='import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#set options\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "#scikit-learn is a huge library. We import what we need.\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, train_test_split #sklearn utilities\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report #For model evaluation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer #Vectorizers\n",
    "from sklearn.linear_model import LogisticRegressionCV #Logit with cross-validation\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier #Random Forest and AdaBoost classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree #Decision Tree classifier\n",
    "from sklearn.svm import LinearSVC #Linear Support Vector classifier\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and inspect dataset<a id='dataset'></a>\n",
    "\n",
    "The dataset was collected by [Crowdflower](https://www.crowdflower.com/), which they then made public through [Kaggle](https://www.kaggle.com/crowdflower/twitter-airline-sentiment). Note that this is a nice clean dataset--not the norm in real-life data science, but it lets us focus on text classification rather than preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../public_data/crowdflower_tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which airlines are tweeted about and how many of each in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['airline'], order=df['airline'].value_counts().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get to know the data:\n",
    "\n",
    "- How many tweets are in the dataset?\n",
    "- How many tweets are positive, neutral and negative?\n",
    "- What **proportion** of tweets are positive, neutral and negative?\n",
    "- What are the main reasons why people are tweeting negatively? \n",
    "\n",
    "To visualize counts, this time let's use the `sns.countplot()` function. To visualize proportions, we'll use the `.plot(kind='bar')` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length is\", len(df))\n",
    "\n",
    "df['airline_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['airline_sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['airline_sentiment'].value_counts(normalize=True, ascending=True).plot(kind='bar', rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['negativereason'], order=df['negativereason'].value_counts().index)\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data<a id='preprocess'></a>\n",
    "\n",
    "How are we going to turn our tweets into numbers? First, let's do some quick preprocessing to remove usernames, hashtags, and URLs. Let's assume we don't need any of this information, so removing such junk will sharpen the signal of the text itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_handle_pattern = r'@(\\w+)'\n",
    "hashtag_pattern = r'(?:^|\\s)[＃#]{1}(\\w+)'\n",
    "url_pattern = r'https?:\\/\\/.*.com'\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(hashtag_pattern, ' HASHTAG', tweet) \n",
    "    tweet = re.sub(twitter_handle_pattern, 'USER', tweet)\n",
    "    return re.sub(url_pattern, 'URL', tweet)\n",
    "\n",
    "# test function on toy tweets\n",
    "my_tweets = [\"lol @justinbeiber and @BillGates are like soo #yesterday #amiright saw it on https://twitter.com #yolo\",\n",
    "            'omg I am never flying on Delta again',\n",
    "            'I love @VirginAmerica so much #friendlystaff']\n",
    "\n",
    "for tweet in my_tweets:\n",
    "    print(clean_tweet(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to DF \n",
    "df['clean_text'] = (df['text'].apply(clean_tweet))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization<a id='vectorization'></a>\n",
    "\n",
    "Now that we've cleaned the text, we turn the text into numbers for our classifier. As we did last class, we will use `scikit-learn`'s `CountVectorizer` and a \"bag of words\" approach to create our features: frequency counts of all the words that appear in a text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvectorizer = CountVectorizer(max_features=5000, binary=True)\n",
    "X = countvectorizer.fit_transform(df['clean_text'])\n",
    "features = X.toarray() # convert matrix to sparse format for easy modeling\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = df['airline_sentiment'].values # corresponds to entries in `features` \n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature names (from vectorizer) and label names, will be useful later\n",
    "feature_names = countvectorizer.get_feature_names() \n",
    "label_names = list(set(response))\n",
    "print(\"Labels:\", label_names)\n",
    "print()\n",
    "print(\"A few features:\", feature_names[500:510])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data into training and test sets<a id='split'></a>\n",
    "\n",
    "As we did last class, let's split our data into training and test sets. _Unlike_ last class, we will use a shortcut to create this split: the useful `train_test_split()` function from `scikit-learn`. We will also balance the outcome variable (sentiment) between our training and test samples. \n",
    "\n",
    "We'll train on 80% of the data and test the model's accuracy on the rest. An 80/20 split is typical in machine learning tasks; according to the [Pareto principle](https://en.wikipedia.org/wiki/Pareto_principle), this ratio allows for efficiency and occurs in nature as well as in business, computing, and economics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, response, test_size=0.2, stratify = df.airline_sentiment)\n",
    "\n",
    "X_train.shape, X_test.shape #look at number of rows and columns in training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've split our data up into `train` and `test` sets, let's look to see how the output variable is distributed within the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,5))\n",
    "plt.subplot(1,2,1) # shows separate plot on left \n",
    "plt.hist(y_train, bins=5)\n",
    "plt.title('Train')\n",
    "\n",
    "plt.subplot(1,2,2) # shows separate plot on right\n",
    "plt.hist(y_test, bins=5);\n",
    "plt.title('Test');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three possible values of the output variable (sentiment) appear more or less equally distributed across the training and test datasets, which matters for model performance. (If they weren't, we could use the `stratify` parameter of `sklearn`'s `train_test_split()` method to split them equally.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More classification with supervised machine learning<a id='supervised'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train decision tree model<a id='model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize decision tree model using scikit-learn\n",
    "dt_classifier = DecisionTreeClassifier(criterion='gini',  # or 'entropy' for information gain\n",
    "                                       splitter='best',  # or 'random' for random best split\n",
    "                                       max_depth=10,  # how deep tree nodes can go\n",
    "                                       min_samples_split=5,  # samples needed to split node\n",
    "                                       min_samples_leaf=10,  # samples needed for a leaf\n",
    "                                       min_weight_fraction_leaf=0.0,  # weight of samples needed for a node\n",
    "                                       max_features=None,  # number of features to look for when splitting\n",
    "                                       max_leaf_nodes=None,  # max nodes\n",
    "                                       min_impurity_decrease=1e-07, #early stopping\n",
    "                                       random_state = 10) #random seed\n",
    "\n",
    "dt_classifier.fit(X_train, y_train) # fit model on training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An advantage of decision tree models is that they are easy to visualize. Let's look at the top four levels of the tree we just built, which show the decision rules the algorithm learned from features to determine the labels. The top part of each bubble indicates the feature being checked for that decision node; for instance, a node checking for $thank <= 0.5$ registers as $True$ when this word is absent and $False$ when the word occurs at least once. In either case, that case may go down another branch and be checked further at later nodes, creating a potential complex of conditions. \n",
    "\n",
    "'gini' refers to the importance of the feature; the higher this value, the more important that feature is for determinining the associated label. 'samples' shows the number of cases that reach that node, while 'class' indicates the label assigned for cases that stop at that decision node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_data = export_graphviz(dt_classifier, \n",
    "                          out_file=None, \n",
    "                          feature_names=feature_names,  \n",
    "                          class_names=y_train,\n",
    "                          max_depth=4,\n",
    "                          filled=True, \n",
    "                          rounded=True,  \n",
    "                          special_characters=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(dt_classifier, \n",
    "                   feature_names=feature_names,  \n",
    "                   class_names=label_names,\n",
    "                   filled=True, )\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "tree.plot_tree(dt_classifier, \n",
    "                   feature_names=feature_names,  \n",
    "                   class_names=label_names,\n",
    "                   filled=True, max_depth=3,fontsize=20,\n",
    "      )\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test model performance, we can use either `accuracy_score()` or the built-in `score` method to return the mean accuracy. Either method can be used on both the train and test datasets. Let's check the in-sample accuracy of the model on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the labels on the training set using the trained model\n",
    "predictions_dt = dt_classifier.predict(X_train) \n",
    "\n",
    "accuracy_score(predictions_dt, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's an excellent score! Unfortunately, it results from **overfitting**--using the same data for model training as for evaluation. To get a better sense of how our model would generalize to unseen data, let's evaluate the performance of this model using the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the built-in `model.score()` method instead this time, results are same as for `accuracy_score()`\n",
    "print(dt_classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite perfect, but more realistic and still good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IN-CLASS CHALLENGE 1: Predict labels on unseen data\n",
    "\n",
    "Use the Decisition Tree model you just built to predict the label for three unlabeled reviews. Predict both the label and the probability of being in each category. Do the predictions make sense? Do the probabilities provide any useful information beyond the predicted labels?\n",
    "\n",
    "_Hint:_ Remember to use `countvectorizer()` and convert to sparse format with `.toarray()` to vectorize the texts prior to prediction. Use `model.predict()` to predict the class and `model.predict_proba()` to predict class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "careless_review = \"I made my flight reservation with UnitedAirlines and the service is ridiculous and they just bluntly \\\n",
    "deny solving your problem. I booked my flight but they were having some technical issues and didn't appear \\\n",
    "in their system. United customer service just said they can not do anything and asked me to book a new ticket to fly. \\\n",
    "This is just insane and troubling and also really costly at the airport. Thus, I made my flight reservation \\\n",
    "with United Airlines for the next day through an online flight booking portal that offered much cheaper flights. \\\n",
    "You can contact them over the phone by calling a toll-free number +1-888-720-1433 and book flights that are light \\\n",
    "on your pocket.\"\n",
    "\n",
    "good_review = \"United was very accommodating for our vacation recently and even got us home a little quicker. \\\n",
    "We were travelling in a group of 12 coming home from a vacation and because of storms back east our connecting flight \\\n",
    "was delayed by hours. Knowing this was completely out of their control we started talking with one of the United \\\n",
    "employees and told them where we were headed. She said she could get us on a flight that actually leaves earlier \\\n",
    "than our original flight. It was such great service and resulted in all of us getting home hours later. The only \\\n",
    "issue was our bags, but United had them delivered to our homes that next morning.\"\n",
    "\n",
    "canceled_review = \"Canceled my trip twice, the first time I received no notification. The second time I got a notice \\\n",
    "that one of my trips may have been canceled or changed, the website didn’t say which it was. It instructed me to cancel \\\n",
    "it myself. Probably so they could avoid having to refund me. Now I all the money I spent on tickets sitting in a United \\\n",
    "account but I never want to fly with them in the future so that’s a nice chunk of about $2000 wasted. \\\n",
    "EDIT: After I cancelled my second trip I was able to select an option for refund, the refund was not only \\\n",
    "for the second trip but also my first trip that was cancelled earlier. I will upgrade the rating from 1 to 3 stars \\\n",
    "would have given higher with better customer service/website service/website information.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation<a id='cv'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing our labeled data into a single set of training and test data gives a limited picture of how our algorithm performs on unseen data. Ideally, we would want a lot of unseen data to test our model on, to reduce the risk that the test data we selected (even if randomly) is unusually similar to or different from the training data in terms of structure. How can we use our limited labeled data to get a better sense of the accuracy of a trained model? \n",
    "\n",
    "This is where **cross-validation** comes in. One way to do this is to keep an additional set of test data from model training AND testing (called _held-out test set_ and usually differentiated from a _validation set_), and only to use it for evaluation after model construction is finalized. However, this leaves even less data available for model training, which can result in **underfitting**. \n",
    "\n",
    "$K$-fold cross-validation can be easily implemented using `scikit-learn`'s `cross_val_score()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(dt_classifier, X_train, y_train, cv=3)\n",
    "scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show average accuracy across folds along with sigma (std. squared)\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the cross validation accuracy different than our classifier? Which estimate is more accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More model evaluation <a id=\"evaluate\"></a>\n",
    "\n",
    "One of the most useful layouts to observe model performance is a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), a table layout showing the relationships between predicted, true, and condition positives and negatives. Columns show predicted outcomes, while rows show ground truth. First let's look at the raw output of `scikit-learn`'s `confusion_matrix()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, dt_classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little hard to interpret, so let's make a prettier table to show model performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dt_classifier # change this one line to visualize confusion matrix for other models\n",
    "y_predict = model.predict(X_test) \n",
    "labels = model.classes_\n",
    "conmat = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "sns.heatmap(conmat, \n",
    "            annot=True, \n",
    "            fmt='d', \n",
    "            xticklabels=labels, \n",
    "            yticklabels=labels, \n",
    "            cmap=\"YlGnBu\", \n",
    "            cbar=False)\n",
    "plt.ylabel('Ground truth')\n",
    "plt.xlabel('Prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better, but what about the different metrics to convey all this information? Above we looked at accuracy, but `scikit-learn` can also print out a lot more information--including the **precision** and **recall** scores for each outcome--of a classification model with the `classification_report()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt_predicted = dt_classifier.predict(X_test)\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, dt_predicted)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize parameters with grid search <a id=\"gridsearch\"></a>\n",
    "\n",
    "Optimizing model parameters is one of the most important steps in building a supervised machine learning model. One way to do this is by using what's called a [grid search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). A grid search tests different possible parameter combinations to see which combination yields the most accurate predictions. Fortunately, `scikit-learn` makes this very easy to do with the `GridSearchCV()` function. \n",
    "\n",
    "Here we'll see what is the best combination of the parameters `min_samples_split` and `min_samples_leaf` for building a decision tree model to predict user sentiment in our airline reviews dataset. We can make a dictionary with the names of the parameters as the keys and the range of values as the corresponding values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid searching is computationally expensive, so for time's sake we'll test a minimal range for each parameter\n",
    "# In real-world data analytics there would be a much larger range, making exponentially more permutations (and time cost)\n",
    "\n",
    "param_grid = {'min_samples_split': [2,10], # define dictionary with possible model parameters\n",
    "              'min_samples_leaf': [2,10]}\n",
    "\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can implement the grid search and fit our model according to the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: This can take a long time!\n",
    "model_dt = GridSearchCV(dt_classifier, \n",
    "                        param_grid, \n",
    "                        cv=3, \n",
    "                        return_train_score=True)\n",
    "\n",
    "model_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what are the model parameters to produce the highest accuracy on the training set, we will find the maximum `mean_train_score` and its associated parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information on model parameters that perform best\n",
    "model = model_dt # Change this to visualize other models\n",
    "\n",
    "# Get index of best model parameters\n",
    "best_index = np.argmax(\n",
    "    model.cv_results_[\"mean_train_score\"]\n",
    "                      )\n",
    "\n",
    "print('Best parameter values are:', \n",
    "      model.cv_results_[\"params\"][best_index]\n",
    "     )\n",
    "print('Best mean cross-validation train accuracy: %.03f' % \n",
    "      model.cv_results_[\"mean_train_score\"][best_index]\n",
    "     )\n",
    "print('Overall mean test accuracy: %.03f' % \n",
    "      model.score(X_test, y_test)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and optimize Random Forest model <a id=\"RF\"></a>\n",
    "\n",
    "Now we'll look at [Random Forests](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), a more complex method that pools decision tree classifiers and averages their results to improve accuracy and reduce overfitting. Each decision tree is fit to a subset of the data (bagging), and uses only a subset of the features (random subspace). Random forests are an example of _ensemble methods_, which pool results from many simpler classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=10,  # number of trees\n",
    "                       criterion='gini',  # or 'entropy' for information gain\n",
    "                       max_depth=10, )  # adjusts weights inverse of freq, also \"balanced_subsample\" or None\n",
    "\n",
    "rf_model = rf_classifier.fit(X_train, y_train) # fit model on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the classification performance on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of Random Forest model (with 3-fold CV) with test data defined above:\")\n",
    "scores = cross_val_score(rf_model, X_test, y_test, cv=3)\n",
    "print(\"%0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2)) # Show average accuracy across folds\n",
    "print()\n",
    "\n",
    "predicted = rf_model.predict(X_test)\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, predicted)) \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IN-CLASS CHALLENGE 2: Grid Search on Random Forest\n",
    "\n",
    "Do another grid search to determine the best parameters for the Random Forest we just created. Use two possible levels for `min_samples_split` and `min_samples_leaf`, each between 5 and 20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IN-CLASS CHALLENGE 3: Adjust text preprocessing\n",
    "\n",
    "Preprocessing methods matter for machine learning performance: Depending on the algorithm, less or more preprocessing may be better. Let's see how a simple Random Forest model does with minimal preprocessing--without removing usernames, hashtags, or URLs. Compare this model's performance with that of Random Forest trained on cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
